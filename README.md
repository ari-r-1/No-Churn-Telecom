# 📌 No-Churn-Telecom: Customer Churn Prediction  

## 📖 Project Overview  
The **No-Churn-Telecom** project predicts customer churn in the telecom sector using machine learning techniques.  
With a dataset of **4,617 customer records**, the goal is to identify customers likely to leave and provide business insights for retention strategies.  

---

## 🗂 Dataset  
- **Size**: 4,617 customer records  
- **Features**: Call durations, number of calls, international plan, voicemail plan, etc.  
- **Target**: `Churn` (Yes/No)  

---

## ⚙️ Methodology  
1. **Data Preprocessing** – Handling missing values, encoding categorical variables, scaling features.  
2. **Exploratory Data Analysis (EDA)** – Understanding churn distribution, correlations, and feature importance.  
3. **Modeling** – Applied multiple algorithms:  
   - Logistic Regression  
   - Random Forest  
   - Gradient Boosting (XGBoost, LGBM)  
4. **Fine-Tuning** – Hyperparameter tuning for optimal performance.  
5. **Evaluation** – Accuracy, precision, recall, F1-score, ROC-AUC.  

---

## 📊 Results  
- **Best Model**: Light Gradient Boosting Machine (LGBM)  
- **Accuracy**: **97.18%**  
- **Insights**: International plans, call usage, and voicemail are strong churn predictors.  

---

## 💡 Business Impact  
- Helps telecom companies **identify high-risk customers**.  
- Supports **data-driven retention strategies**.  
- Reduces revenue loss by minimizing churn.  

---

## 🛠 Tools & Technologies  
- **Python**  
- **Pandas, NumPy** – Data handling  
- **Matplotlib, Seaborn** – Data visualization  
- **Scikit-learn** – Machine learning models  
- **LightGBM, XGBoost** – Gradient boosting algorithms  

---
## 📈 Workflow

1. **Exploratory Data Analysis (EDA)**  
   - Distribution of features  
   - Correlation analysis  
   - Handling missing values & class imbalance  

2. **Data Preprocessing**  
   - Encoding categorical variables  
   - Feature scaling  
   - Handling imbalanced classes (SMOTE/undersampling)  

3. **Modeling**  
   - Logistic Regression  
   - Random Forest  
   - XGBoost  
   - Support Vector Machine  

4. **Model Evaluation**  
   - Train-test split & cross-validation  
   - Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC  
   - Comparison of models  

5. **Insights & Business Value**  
   - Identifying customer segments most likely to subscribe  
   - Recommendations for improving campaign targeting  

---

## 📊 Visualizations

- Feature importance plots  
- Correlation heatmaps  
- ROC & Precision-Recall curves  
- Confusion matrices  

---

## 💡 Future Improvements

- Deploy as a Flask/Django web app  
- Integrate with real-time campaign data  
- Use deep learning models for comparison  
- Hyperparameter optimization with Optuna  

---

## 🤝 Contributing

**Ari R.**  
_Data Scientist_  
🔗 [GitHub](https://github.com/ari-r-1) | 📧 ariranalyst@gmail.com

---

✨ Developed with passion for Data Science & Machine Learning.
"# No-Churn-Telecom" "# No-Churn-Telecom" 
